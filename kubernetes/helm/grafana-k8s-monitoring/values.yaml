cluster:
  name: potato

destinations:
  - name: grafana-cloud-metrics
    type: prometheus
    url: $GRAFANA_METRICS_URL
    auth:
      type: basic
      username: "$GRAFANA_METRICS_USER"
      password: $GRAFANA_ACCESS_TOKEN
  - name: grafana-cloud-logs
    type: loki
    # make sure 'k8s.cluster.name' resource attribute is used as label
    # it's otlp convention and using unindexed cluster label breaks
    # traces to logs correlation
    # https://grafana.com/docs/loki/latest/get-started/labels/#default-labels-for-opentelemetry
    # https://github.com/grafana/k8s-monitoring-helm/blob/main/charts/k8s-monitoring/destinations/loki-values.yaml#L58-L60
    clusterLabels: [k8s.cluster.name]
    url: $GRAFANA_LOGS_URL
    auth:
      type: basic
      username: "$GRAFANA_LOGS_USER"
      password: $GRAFANA_ACCESS_TOKEN
  - name: grafana-cloud-otlp-endpoint
    type: otlp
    # make sure 'k8s.cluster.name' resource attribute is used as label
    # it's otlp convention and using unindexed cluster label breaks
    # traces to logs correlation
    # https://grafana.com/docs/loki/latest/get-started/labels/#default-labels-for-opentelemetry
    # https://github.com/grafana/k8s-monitoring-helm/blob/main/charts/k8s-monitoring/destinations/otlp-values.yaml#L57-L59
    clusterLabels: [k8s.cluster.name]
    url: $GRAFANA_OTLP_URL
    protocol: http
    auth:
      type: basic
      username: "$GRAFANA_OTLP_USER"
      password: $GRAFANA_ACCESS_TOKEN
    # processors:
    #   transform:
    #     logs:
    #       # https://github.com/grafana/k8s-monitoring-helm/blob/main/charts/k8s-monitoring/destinations/otlp-values.yaml#L315-L334
    #       logToResource:
    #         cluster: k8s.cluster.name
    #     # https://github.com/grafana/k8s-monitoring-helm/blob/main/charts/k8s-monitoring/destinations/otlp-values.yaml#L223-L228
    #     tailSampling:
    #       # -- Apply tail sampling policies to the traces before delivering them to this destination. This will create an
    #       # additional Alloy instance to handle the tail sampling, and traces sent to this destination will be automatically
    #       # forwarded, using a load balancer component, to the new sampling Alloy instance.
    #       # @section -- Tail Sampling
    #       enabled: false

    metrics:
      enabled: true
    logs:
      enabled: true
    traces:
      enabled: true
  - name: grafana-cloud-profiles
    type: pyroscope
    url: $GRAFANA_PROFILES_URL
    auth:
      type: basic
      username: "$GRAFANA_PROFILES_USER"
      password: $GRAFANA_ACCESS_TOKEN

clusterMetrics:
  enabled: true
  opencost:
    enabled: true
    metricsSource: grafana-cloud-metrics
    opencost:
      exporter:
        defaultClusterId: potato
      prometheus:
        existingSecretName: grafana-cloud-metrics-grafana-k8s-monitoring
        external:
          url: $GRAFANA_CLUSTER_METRICS_URL
  kepler:
    enabled: true

annotationAutodiscovery:
  enabled: true

prometheusOperatorObjects:
  enabled: false

clusterEvents:
  enabled: true

nodeLogs:
  enabled: true
  journal:
    path: /run/log/journal

podLogs:
  enabled: true
  excludeNamespaces:
    # exclude otel-demo
    # to avoid confusion we only want logs sent through alloy-receiver
    - otel-demo
    - otel-demo-local

applicationObservability:
  enabled: true
  receivers:
    otlp:
      grpc:
        enabled: true
        port: 4317
  connectors:
    grafanaCloudMetrics:
      enabled: true
  # processors:
  #   attributes:
  #     actions:
  #       # remove cluster label to avoid traces to logs problem
  #       # as 'cluster' is not indexed, 'k8s.cluster.name' is
  #       - key: cluster
  #         action: delete
  # transform:
  #   logs:
  #     resources:
  #       - delete_key(attributes, "cluster")

# careful when running beyla alongside otel-demo
autoInstrumentation:
  enabled: true
  preset: application
  beyla:
    config:
      data:
        attributes:
          kubernetes:
            cluster_name: potato
            enable: true
        # https://grafana.com/docs/beyla/latest/configure/service-discovery/
        discovery:
          exclude_otel_instrumented_services: false
          exclude_services:
          - exe_path: .*alloy.*|.*otelcol.*|.*beyla.*
          # filter out kube-system, monitoring, otel-demo
          - k8s_namespace: .*kube.*|.*monitoring.*|.*otel-demo.*
          services:
          # include hello-api
          - k8s_namespace: .*hello-api.*
        filter:
          network:
            k8s_dst_owner_name:
              not_match: '{kube*,*jaeger-agent*,*prometheus*,*promtail*,*grafana-agent*}'
            k8s_src_owner_name:
              not_match: '{kube*,*jaeger-agent*,*prometheus*,*promtail*,*grafana-agent*}'
        internal_metrics:
          prometheus:
            path: /internal/metrics
            port: 9090
        otel_traces_export:
          endpoint: http://grafana-k8s-monitoring-alloy-receiver.monitoring.svc.cluster.local:4317
          # https://grafana.com/docs/beyla/latest/configure/sample-traces/
          sampler:
            name: "traceidratio"
            arg: "0.1"
        prometheus_export:
          features:
          - application
          - network
          - application_service_graph
          - application_span
          path: /metrics
          port: 9090

profiling:
  enabled: true

integrations:
  alloy:
    instances:
      - name: alloy
        labelSelectors:
          app.kubernetes.io/name:
            - alloy-metrics
            - alloy-singleton
            - alloy-logs
            - alloy-receiver
            - alloy-profiles

alloy-metrics:
  enabled: true

alloy-singleton:
  enabled: true

alloy-logs:
  enabled: true
  # liveDebugging:
  #   enabled: true
  alloy:
    mounts:
      extra:
        - name: var-log-journal
          mountPath: /run/log/journal
  controller:
    volumes:
      extra:
        - name: var-log-journal
          hostPath:
            path: /run/log/journal
            type: DirectoryOrCreate

alloy-receiver:
  enabled: true
  # liveDebugging:
  #   enabled: true
  alloy:
    extraPorts:
      - name: otlp-grpc
        port: 4317
        targetPort: 4317
        protocol: TCP
      - name: otlp-http
        port: 4318
        targetPort: 4318
        protocol: TCP
      - name: zipkin
        port: 9411
        targetPort: 9411
        protocol: TCP

alloy-profiles:
  enabled: true
